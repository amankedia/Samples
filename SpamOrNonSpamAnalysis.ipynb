{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SpamOrNonSpamAnalysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spam filtering is an example of document classification task which involves classifying an email / SMS as spam or non-spam (a.k.a. ham).\n",
    "\n",
    "1. Import the dataset from https://www.kaggle.com/uciml/sms-spam-collection-dataset. (1 point).\n",
    "2. Split the data into training and testing. (1 point). Use 10-fold cross validation.(1 point)\n",
    "3. Extract features using TF-IDF and display the features. ( 2 points)\n",
    "4. Model and train the classifier using GaussianNB, BernoulliNB and MultinomialNB.( 3 points)\n",
    "5. Evaluate classifiers on Test Data. ( 2 points)\n",
    "6. Plot the decision boundary, visualize training and test results of all the models (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re, nltk\n",
    "from itertools import product\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, average_precision_score\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('spam.csv', encoding = 'ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     v1                                                 v2 Unnamed: 2  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...        NaN   \n",
       "1   ham                      Ok lar... Joking wif u oni...        NaN   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n",
       "3   ham  U dun say so early hor... U c already then say...        NaN   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n",
       "\n",
       "  Unnamed: 3 Unnamed: 4  \n",
       "0        NaN        NaN  \n",
       "1        NaN        NaN  \n",
       "2        NaN        NaN  \n",
       "3        NaN        NaN  \n",
       "4        NaN        NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding statistics of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5572</td>\n",
       "      <td>5572</td>\n",
       "      <td>50</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>2</td>\n",
       "      <td>5169</td>\n",
       "      <td>43</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>ham</td>\n",
       "      <td>Sorry, I'll call later</td>\n",
       "      <td>bt not his girlfrnd... G o o d n i g h t . . .@\"</td>\n",
       "      <td>GE</td>\n",
       "      <td>GNT:-)\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>4825</td>\n",
       "      <td>30</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          v1                      v2  \\\n",
       "count   5572                    5572   \n",
       "unique     2                    5169   \n",
       "top      ham  Sorry, I'll call later   \n",
       "freq    4825                      30   \n",
       "\n",
       "                                               Unnamed: 2 Unnamed: 3  \\\n",
       "count                                                  50         12   \n",
       "unique                                                 43         10   \n",
       "top      bt not his girlfrnd... G o o d n i g h t . . .@\"         GE   \n",
       "freq                                                    3          2   \n",
       "\n",
       "       Unnamed: 4  \n",
       "count           6  \n",
       "unique          5  \n",
       "top       GNT:-)\"  \n",
       "freq            2  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop the columns which are not significant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'],  axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding statistics of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5572</td>\n",
       "      <td>5572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>2</td>\n",
       "      <td>5169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>ham</td>\n",
       "      <td>Sorry, I'll call later</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>4825</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          v1                      v2\n",
       "count   5572                    5572\n",
       "unique     2                    5169\n",
       "top      ham  Sorry, I'll call later\n",
       "freq    4825                      30"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert the categorical class Attribute to numerical Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "le.fit(data['v1'])\n",
    "data['v1'] = le.transform(data.v1.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5572.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.134063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.340751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                v1\n",
       "count  5572.000000\n",
       "mean      0.134063\n",
       "std       0.340751\n",
       "min       0.000000\n",
       "25%       0.000000\n",
       "50%       0.000000\n",
       "75%       0.000000\n",
       "max       1.000000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/pallssin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/pallssin/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer \n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Purpose : Function to keep only alphabets, digits and certain words (punctuations, question marks, tabs etc. removed) \n",
    "### Input : Takes a text corpus, 'corpus' to be cleaned along with a list of words, 'keep_list', which have to be retained even after the cleaning process\n",
    "### Output : Returns the cleaned text corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_clean(corpus, keep_list):\n",
    "    cleaned_corpus = pd.Series()\n",
    "    for row in corpus:\n",
    "        qs = []\n",
    "        for word in row.split():\n",
    "            if word not in keep_list:\n",
    "                p1 = re.sub(pattern='[^a-zA-Z0-9]',repl=' ',string=word)\n",
    "                p1 = p1.lower()\n",
    "                qs.append(p1)\n",
    "            else : qs.append(word)\n",
    "        cleaned_corpus = cleaned_corpus.append(pd.Series(' '.join(qs)))\n",
    "    return cleaned_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Purpose : Function to perform all pre-processing tasks (cleaning, stemming, lemmatization, stopwords removal          etc.)\n",
    "### Input : \n",
    "### 'corpus' - Text corpus on which pre-processing tasks will be performed\n",
    "### 'keep_list' - List of words to be retained during cleaning process\n",
    "### 'cleaning', 'stemming', 'lemmatization', 'remove_stopwords' - Boolean variables indicating whether a particular task should be performed or not\n",
    "### 'stem_type' - Choose between Porter stemmer or Snowball(Porter2) stemmer. Default is \"None\", which corresponds to Porter Stemmer. 'snowball' corresponds to Snowball Stemmer \n",
    "### Note : Either stemming or lemmatization should be used. There's no benefit of using both of them together\n",
    "### Output : Returns the processed text corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(corpus, keep_list, cleaning = True, stemming = True, stem_type = None, lemmatization = True, remove_stopwords = True):\n",
    "\n",
    "    if cleaning == True:\n",
    "        corpus = text_clean(corpus, keep_list)\n",
    "    \n",
    "    if remove_stopwords == True:\n",
    "        wh_words = ['who', 'what', 'when', 'why', 'how', 'which', 'where', 'whom']\n",
    "        stop = set(stopwords.words('english'))\n",
    "        for word in wh_words:\n",
    "            stop.remove(word)\n",
    "        corpus = [[x for x in x.split() if x not in stop] for x in corpus]\n",
    "    else :\n",
    "        corpus = [[x for x in x.split()] for x in corpus]\n",
    "    \n",
    "    if lemmatization == True:\n",
    "        lem = WordNetLemmatizer()\n",
    "        corpus = [[lem.lemmatize(x, pos = 'v') for x in x] for x in corpus]\n",
    "    \n",
    "    if stemming == True:\n",
    "        if stem_type == 'snowball':\n",
    "            stemmer = SnowballStemmer(language = 'english')\n",
    "            corpus = [[stemmer.stem(x) for x in x] for x in corpus]\n",
    "        else :\n",
    "            stemmer = PorterStemmer()\n",
    "            corpus = [[stemmer.stem(x) for x in x] for x in corpus]\n",
    "    \n",
    "    corpus = [' '.join(x) for x in corpus]\n",
    "        \n",
    "\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5572.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.134063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.340751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                v1\n",
       "count  5572.000000\n",
       "mean      0.134063\n",
       "std       0.340751\n",
       "min       0.000000\n",
       "25%       0.000000\n",
       "50%       0.000000\n",
       "75%       0.000000\n",
       "max       1.000000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preforming cleaning, stemming, lemmatization and removing stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_dot_words = ['U.S.', 'St.', 'Mr.', 'Mrs.', 'D.C.']\n",
    "preprocessed_data = preprocess(data['v2'], keep_list = common_dot_words, remove_stopwords = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['go jurong point crazi avail bugi n great world la e buffet cine get amor wat',\n",
       " 'ok lar joke wif u oni',\n",
       " 'free entri 2 wkli comp win fa cup final tkt 21st may 2005 text fa 87121 receiv entri question std txt rate c appli 08452810075over18',\n",
       " 'u dun say earli hor u c alreadi say',\n",
       " 'nah think go usf live around though',\n",
       " 'freemsg hey darl 3 week word back like fun still tb ok xxx std chg send 1 50 rcv',\n",
       " 'even brother like speak treat like aid patent',\n",
       " 'per request mell mell oru minnaminungint nurungu vettam set callertun caller press 9 copi friend callertun',\n",
       " 'winner valu network custom select receivea 900 prize reward claim call 09061701461 claim code kl341 valid 12 hour',\n",
       " 'mobil 11 month u r entitl updat latest colour mobil camera free call mobil updat co free 08002986030']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = data['v1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    0\n",
       "2    1\n",
       "3    0\n",
       "4    0\n",
       "Name: v1, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(preprocessed_data)\n",
    "X = X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count                 5572\n",
       "unique                5112\n",
       "top       sorri call later\n",
       "freq                    30\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the dataset into the Training set and Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=0, stratify = Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1257                                         also cbe pay\n",
       "5461                          ok thk get u wan 2 come wat\n",
       "1612    rt king pro video club need help info ringtone...\n",
       "2179                             pop lt gt ibuprofen help\n",
       "2638                                      gobi art colleg\n",
       "                              ...                        \n",
       "3297    messag free welcom new improv sex dog club uns...\n",
       "1054    hiya comin 2 bristol 1 st week april le get ru...\n",
       "245                          late say websit dont slipper\n",
       "1235    opinion 1 2 jada 3 kusruthi 4 lovabl 5 silent ...\n",
       "3361                                    messag phone hold\n",
       "Name: 0, Length: 4457, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the training data on TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4457, 6296)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tranforming the test data on the built TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1115, 6296)\n"
     ]
    }
   ],
   "source": [
    "X_test = vectorizer.transform(X_test)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10-fold cross validation and GridSearchCV to Estimate Best Parameters\n",
    "## Fitting  GaussianNB to the Training set and Predicting on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score=nan,\n",
       "             estimator=GaussianNB(priors=None, var_smoothing=1e-09),\n",
       "             iid='deprecated', n_jobs=None,\n",
       "             param_grid={'var_smoothing': [1e-09, 7e-10, 4e-10, 2e-10, 1e-10,\n",
       "                                           9e-11, 8e-11, 7e-11, 6e-11, 3e-11,\n",
       "                                           1e-11]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = {\n",
    "    'var_smoothing': [1e-9, 0.7e-9, 0.4e-9, 0.2e-9, 0.1e-9, 0.09e-9, 0.08e-9, 0.07e-9, 0.06e-9, 0.03e-9, 0.01e-9] \n",
    "}\n",
    "\n",
    "grid = GridSearchCV(clf, param_grid = parameters, cv=10, refit=True)\n",
    "grid.fit(X_train.toarray(), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8682853831813373\n",
      "{'var_smoothing': 1e-09}\n"
     ]
    }
   ],
   "source": [
    "print(grid.best_score_)\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8672645739910314"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.score(X_test.toarray(), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting  MultinomialNB to the Training set and Predicting on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score=nan,\n",
       "             estimator=MultinomialNB(alpha=1.0, class_prior=None,\n",
       "                                     fit_prior=True),\n",
       "             iid='deprecated', n_jobs=None,\n",
       "             param_grid={'alpha': [1, 0.7, 0.4, 0.2, 0.1, 0.09, 0.08, 0.07,\n",
       "                                   0.06, 0.03, 0.01]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = {\n",
    "    'alpha': [1, 0.7, 0.4, 0.2, 0.1, 0.09, 0.08, 0.07, 0.06, 0.03, 0.01] \n",
    "}\n",
    "\n",
    "grid = GridSearchCV(clf, param_grid = parameters, cv=10, refit=True)\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9858648662266336\n",
      "{'alpha': 0.08}\n"
     ]
    }
   ],
   "source": [
    "print(grid.best_score_)\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9757847533632287"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.score(X_test.toarray(), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting  BernoulliNB to the Training set and Predicting on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = BernoulliNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score=nan,\n",
       "             estimator=BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None,\n",
       "                                   fit_prior=True),\n",
       "             iid='deprecated', n_jobs=None,\n",
       "             param_grid={'alpha': [1, 0.7, 0.4, 0.2, 0.1, 0.09, 0.08, 0.07,\n",
       "                                   0.06, 0.03, 0.01]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = {\n",
    "    'alpha': [1, 0.7, 0.4, 0.2, 0.1, 0.09, 0.08, 0.07, 0.06, 0.03, 0.01] \n",
    "}\n",
    "\n",
    "grid = GridSearchCV(clf, param_grid = parameters, cv=10, refit=True)\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9896795485463799\n",
      "{'alpha': 0.1}\n"
     ]
    }
   ],
   "source": [
    "print(grid.best_score_)\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9820627802690582"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.score(X_test.toarray(), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting decision regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1.0, 1.7000000000000024)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "X_set, y_set = X_train[:, :2], y_train\n",
    "clf.fit(X_set, y_set)\n",
    "\n",
    "X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\n",
    "                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\n",
    "plt.contourf(X1, X2, clf.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\n",
    "             alpha = 0.75, cmap = ListedColormap(('red', 'green')))\n",
    "plt.xlim(X1.min(), X1.max())\n",
    "plt.ylim(X2.min(), X2.max())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
